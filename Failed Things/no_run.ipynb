{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0912bdfb-c759-4191-b95d-e3ca95ba4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d6a51-c9df-4ae3-913b-aa6d9f6acd4f",
   "metadata": {},
   "source": [
    "<h4>Step1: Dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560d79b5-03c3-44a5-a717-741f7071163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feeb3f6f-5e7a-4b91-8352-7e06ab7ea96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "data_dir = './dataset/asl_alphabet_train/asl_alphabet_train/'\n",
    "\n",
    "# Initialize lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Set the desired image dimensions\n",
    "IMG_SIZE = 224  # Adjust as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16e619-653d-4443-aafd-d439e4b92493",
   "metadata": {},
   "source": [
    "<h4>Step2: Pre-process the data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ba2186-a0fb-4721-8d0d-d16406fdf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing or corrupted images\n",
    "def is_valid_image(img_path):\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return False  # File is corrupted or unreadable\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2477725-e176-4a99-a946-4b683cb0e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(file_path, folder):\n",
    "    if is_valid_image(file_path):\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.astype('float32') / 255.0\n",
    "        return image, folder\n",
    "    else:\n",
    "        print(f\"Warning: Skipping corrupted image {file_path}\")\n",
    "        return None, None\n",
    "\n",
    "def load_images(data_dir):\n",
    "    data, labels = [], []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for folder in os.listdir(data_dir):\n",
    "            folder_path = os.path.join(data_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for file in os.listdir(folder_path):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    futures.append(executor.submit(process_image, file_path, folder))\n",
    "        \n",
    "        # Collect results\n",
    "        for future in futures:\n",
    "            image, label = future.result()\n",
    "            if image is not None:\n",
    "                data.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19356263-ecef-45dc-8b60-795eea5598d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process the data concurrent\n",
    "data, labels = load_images(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cc2c8-274f-42c6-aa46-41cc40130156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a73b1-288c-4926-bfb6-91985603af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle potential missing labels (if there are any)\n",
    "valid_data_indices = ~np.isnan(labels)  # If labels are numeric, adapt if needed\n",
    "data = data[valid_data_indices]\n",
    "labels = labels[valid_data_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaca943-4234-4cb5-9bf9-4ad8cae50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5934ae1-29ba-44a8-b24d-c12a9305f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592742f-2127-4e8f-a49b-4b3897ff8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of dataset shape\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65cc0b-9286-4043-9e2b-3a1b4a6b4eea",
   "metadata": {},
   "source": [
    "<h4>Step3: Data Exploration</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c7038-e3bd-4247-ac42-06aca22f1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check distribution of classes (labels)\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=list(class_distribution.keys()), y=list(class_distribution.values()))\n",
    "plt.title('Class Distribution (ASL Letters)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4a6a1-1775-41a7-9a62-2b92c52258d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize sample images from each class\n",
    "def visualize_samples_per_class(data, labels, num_samples=5):\n",
    "    unique_classes = np.unique(labels)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    for i, label in enumerate(unique_classes):\n",
    "        class_indices = np.where(np.argmax(labels, axis=1) == i)[0]\n",
    "        sample_images = data[class_indices][:num_samples]\n",
    "        \n",
    "        for j in range(num_samples):\n",
    "            plt.subplot(len(unique_classes), num_samples, i * num_samples + j + 1)\n",
    "            plt.imshow(sample_images[j])\n",
    "            plt.title(f\"Class: {label}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples_per_class(X_train, y_train, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fe089-6887-4c0a-9774-f82d54a6ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyze pixel value distributions for all images (are they normally distributed?)\n",
    "flattened_images = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(flattened_images.flatten(), bins=50, kde=True)\n",
    "plt.title('Pixel Value Distribution')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94add0b7-07ba-4aa2-ac6f-64ad2e34d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check for outliers in pixel intensities (before normalization)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x=flattened_images.flatten())\n",
    "plt.title('Pixel Value Outliers (Boxplot)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f0815-580c-4148-a056-a65cd83c975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualizing correlation of pixel positions (Optional but advanced for image analysis)\n",
    "# Here you can use methods like PCA or t-SNE for dimensionality reduction to visualize correlations\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape the data for PCA (flattening images)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_flat)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=np.argmax(y_train, axis=1), cmap='tab10', alpha=0.6)\n",
    "plt.colorbar()\n",
    "plt.title('PCA of ASL Alphabet Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac339339-61ea-4d9a-b2cc-d57c41dc3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics (mean, std, etc.)\n",
    "print(f\"Mean pixel value: {np.mean(flattened_images)}\")\n",
    "print(f\"Median pixel value: {np.median(flattened_images)}\")\n",
    "print(f\"Standard deviation of pixel values: {np.std(flattened_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80671d39-ecb1-4eb1-b23c-f172db7a998f",
   "metadata": {},
   "source": [
    "<h4>Step4: Model Building</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4593f-7487-46d7-8e81-1af71b70a61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
