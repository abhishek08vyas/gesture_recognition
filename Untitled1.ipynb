{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3b8e09-a8eb-446f-b7e3-571f00971d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.13.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
      "Torch version 2.4.1+cu118 has not been tested with coremltools. You may run into unexpected errors. Torch 2.4.0 is the most recent version that has been tested.\n",
      "Fail to import BlobReader from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Fail to import BlobWriter from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76baac56-882a-4174-9d5c-2fe0cdbd0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLRecognizerCoreML:\n",
    "    def __init__(self, \n",
    "                 model_path='asl_recognition_model811.mlpackage', \n",
    "                 scaler_params_path='scaler_params.json',\n",
    "                 mappings_path='label_mappings.json'):\n",
    "        # Initialize MediaPipe\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # Load the CoreML model\n",
    "        self.model = ct.models.MLModel(model_path)\n",
    "        print(\"Model input description:\", self.model.input_description)\n",
    "        print(\"Model output description:\", self.model.output_description)\n",
    "\n",
    "        # Load scaler parameters\n",
    "        with open(scaler_params_path, 'r') as f:\n",
    "            self.scaler_params = json.load(f)\n",
    "\n",
    "        # Load label mappings\n",
    "        with open(mappings_path, 'r') as f:\n",
    "            mappings = json.load(f)\n",
    "            self.reverse_label_map = {int(k): v for k, v in mappings['reverse_label_map'].items()}\n",
    "\n",
    "        # Initialize display parameters\n",
    "        self.font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.colors = {\n",
    "            'blue': (255, 0, 0),\n",
    "            'green': (0, 255, 0),\n",
    "            'red': (0, 0, 255),\n",
    "            'white': (255, 255, 255)\n",
    "        }\n",
    "\n",
    "    def scale_landmarks(self, landmarks):\n",
    "        \"\"\"Scale the landmarks using the stored scaler parameters.\"\"\"\n",
    "        data_min = np.array(self.scaler_params['data_min_'])\n",
    "        data_range = np.array(self.scaler_params['data_range_'])\n",
    "        scaled = (landmarks - data_min) / data_range\n",
    "        return scaled\n",
    "\n",
    "    def extract_landmarks(self, image):\n",
    "        \"\"\"Extract hand landmarks from image.\"\"\"\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(image_rgb)\n",
    "        \n",
    "        landmarks = None\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Get landmarks of the first hand\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            # Convert to array\n",
    "            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "            \n",
    "            # Draw landmarks on the image\n",
    "            self.mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                self.mp_hands.HAND_CONNECTIONS,\n",
    "                self.mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                self.mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "        \n",
    "        return landmarks, image\n",
    "\n",
    "    def predict(self, landmarks):\n",
    "        \"\"\"Make prediction using the CoreML model.\"\"\"\n",
    "        # Scale landmarks\n",
    "        scaled_landmarks = self.scale_landmarks(landmarks)\n",
    "        # Reshape for model input (batch_size, 21, 3)\n",
    "        input_data = scaled_landmarks.reshape(1, 21, 3)\n",
    "        \n",
    "        # Convert to float32 for CoreML\n",
    "        input_data = input_data.astype(np.float32)\n",
    "        \n",
    "        # Get the input feature name from the model\n",
    "        input_feature_name = self.model.input_description.keys()[0]\n",
    "        \n",
    "        # Prepare input dictionary for CoreML\n",
    "        input_dict = {input_feature_name: input_data}\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(input_dict)\n",
    "        \n",
    "        # Get the output feature name from the model\n",
    "        output_feature_name = self.model.output_description.keys()[0]\n",
    "        prediction_array = prediction[output_feature_name]\n",
    "        \n",
    "        predicted_class = np.argmax(prediction_array)\n",
    "        confidence = np.max(prediction_array)\n",
    "        \n",
    "        return self.reverse_label_map[predicted_class], confidence\n",
    "\n",
    "    def add_prediction_text(self, image, prediction, confidence):\n",
    "        \"\"\"Add prediction text to the image.\"\"\"\n",
    "        # Create background rectangle for text\n",
    "        text = f\"{prediction.upper()}: {confidence:.2f}\"\n",
    "        text_size = cv2.getTextSize(text, self.font, 1, 2)[0]\n",
    "        text_x = 10\n",
    "        text_y = 50\n",
    "        \n",
    "        cv2.rectangle(image, \n",
    "                     (text_x - 5, text_y - text_size[1] - 5),\n",
    "                     (text_x + text_size[0] + 5, text_y + 5),\n",
    "                     self.colors['blue'],\n",
    "                     -1)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(image, text,\n",
    "                    (text_x, text_y),\n",
    "                    self.font, 1, self.colors['white'], 2)\n",
    "\n",
    "    def add_fps_counter(self, image, fps):\n",
    "        \"\"\"Add FPS counter to the image.\"\"\"\n",
    "        cv2.putText(image, f\"FPS: {fps:.1f}\",\n",
    "                    (10, 90),\n",
    "                    self.font, 0.7, self.colors['green'], 2)\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame.\"\"\"\n",
    "        # Extract landmarks\n",
    "        landmarks, annotated_frame = self.extract_landmarks(frame)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            try:\n",
    "                # Make prediction\n",
    "                prediction, confidence = self.predict(landmarks)\n",
    "                # Add prediction text\n",
    "                self.add_prediction_text(annotated_frame, prediction, confidence)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {str(e)}\")\n",
    "        \n",
    "        return annotated_frame\n",
    "\n",
    "    def run_webcam(self):\n",
    "        \"\"\"Run real-time recognition using webcam.\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        # FPS calculation variables\n",
    "        fps = 0\n",
    "        frame_time = 0\n",
    "        prev_time = cv2.getTickCount()\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Flip frame horizontally for selfie-view\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Process frame\n",
    "            output_frame = self.process_frame(frame)\n",
    "            \n",
    "            # Calculate FPS\n",
    "            curr_time = cv2.getTickCount()\n",
    "            frame_time = (curr_time - prev_time) / cv2.getTickFrequency()\n",
    "            fps = 1.0 / frame_time\n",
    "            prev_time = curr_time\n",
    "            \n",
    "            # Add FPS counter\n",
    "            self.add_fps_counter(output_frame, fps)\n",
    "            \n",
    "            # Display result\n",
    "            cv2.imshow('ASL Recognition (CoreML)', output_frame)\n",
    "            \n",
    "            # Break loop on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Clean up\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec5b084-3121-4dfb-a870-f6bb23b1443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Unable to load libmodelpackage. Cannot make save spec.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize and run the recognizer\n",
    "    recognizer = ASLRecognizerCoreML()\n",
    "    recognizer.run_webcam()\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "finally:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430c036-fec5-4659-930f-56a2178f5ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
