{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa3fa3f-f6b1-481f-a6a2-617739c8c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f00bc8-b00e-4609-a8c8-a15ebe972753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf0479dc-14e8-4319-9969-89ecb0cc39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess image for the model\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        img = cv2.resize(img, target_size)\n",
    "        # Convert to RGB and ensure type is uint8\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.uint8)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38c543fc-c708-40a7-9deb-3c946ea5b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(image):\n",
    "    \"\"\"Extract hand landmarks using MediaPipe Holistic\"\"\"\n",
    "    with mp_holistic.Holistic(static_image_mode=True, min_detection_confidence=0.5) as holistic:\n",
    "        # Make sure image is uint8\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "            \n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        landmarks = []\n",
    "        \n",
    "        # Right hand landmarks\n",
    "        if results.right_hand_landmarks:\n",
    "            for landmark in results.right_hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "        else:\n",
    "            landmarks.extend([0.0] * (21 * 3))\n",
    "        \n",
    "        # Left hand landmarks\n",
    "        if results.left_hand_landmarks:\n",
    "            for landmark in results.left_hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "        else:\n",
    "            landmarks.extend([0.0] * (21 * 3))\n",
    "        \n",
    "        return np.array(landmarks, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fe4f6f8-d0c0-4ab8-8adc-538bb4c371f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(args):\n",
    "    \"\"\"Process a single image and return its landmarks and label\"\"\"\n",
    "    image_path, label = args\n",
    "    image = load_and_preprocess_image(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    landmarks = extract_landmarks(image)\n",
    "    return landmarks, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d087da2-3bd2-461c-bead-c4c8f3933220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_dir, max_samples_per_class=1000, num_workers=16):\n",
    "    \"\"\"Prepare dataset by extracting landmarks and creating labels using parallel processing\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    \n",
    "    # First, collect all image paths and labels\n",
    "    for idx, class_name in enumerate(sorted(os.listdir(data_dir))):\n",
    "        if class_name.startswith('.'):  # Skip hidden files\n",
    "            continue\n",
    "        \n",
    "        label_map[idx] = class_name\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        \n",
    "        # Get list of all images in the class directory\n",
    "        class_images = [f for f in os.listdir(class_path) \n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Limit the number of samples per class\n",
    "        if max_samples_per_class:\n",
    "            class_images = class_images[:max_samples_per_class]\n",
    "        \n",
    "        for image_name in class_images:\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(idx)\n",
    "    \n",
    "    print(f\"Processing {len(image_paths)} images across {len(label_map)} classes...\")\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    process_args = list(zip(image_paths, labels))\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Process images in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_single_image, arg) for arg in process_args]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing images\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                landmarks, label = result\n",
    "                X.append(landmarks)\n",
    "                y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.int32)\n",
    "    \n",
    "    return X, y, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ddaeaea-a2f1-4d34-ae9e-576cf570eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    \"\"\"Create a simple neural network model\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f89844b-1b5f-42f2-9201-505d44874b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_model(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=(input_shape,))\n",
    "    \n",
    "    # Batch Normalization at the input\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    # First dense block\n",
    "    x = tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Second dense block\n",
    "    x = tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Third dense block\n",
    "    x = tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2146f0e7-c7a3-44ac-9914-1521853f86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_model(X_train, y_train, X_test, y_test, num_classes, epochs=150):\n",
    "    # Data preprocessing\n",
    "    # Normalize the input data\n",
    "    X_train_mean = X_train.mean(axis=0)\n",
    "    X_train_std = X_train.std(axis=0)\n",
    "    X_train_normalized = (X_train - X_train_mean) / (X_train_std + 1e-7)\n",
    "    X_test_normalized = (X_test - X_train_mean) / (X_train_std + 1e-7)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_enhanced_model(X_train.shape[1], num_classes)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps, decay_rate\n",
    "    )\n",
    "    \n",
    "    # Compile model with learning rate schedule\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_normalized, y_train,\n",
    "        validation_data=(X_test_normalized, y_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f25fac7e-9a3b-4232-87d0-162d4dba8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9000 images across 9 classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|███████████████████████████████████████████████████████████| 9000/9000 [33:25<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared: 9000 samples, 9 classes\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './dataset/asl_alphabet_train/train/'\n",
    "RANDOM_SEED = 42\n",
    "MAX_SAMPLES_PER_CLASS = 1000  # Limit samples per class for faster processing\n",
    "\n",
    "# Prepare dataset with parallel processing\n",
    "X, y, label_map = prepare_dataset(DATA_DIR, max_samples_per_class=MAX_SAMPLES_PER_CLASS)\n",
    "num_classes = len(label_map)\n",
    "\n",
    "print(f\"Dataset prepared: {X.shape[0]} samples, {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5841f984-dc4f-4882-90c7-3f63c49707cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68fb06c3-55e8-4367-a733-2acd334313e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 6.7828 - accuracy: 0.1056\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10611, saving model to best_model.h5\n",
      "225/225 [==============================] - 15s 56ms/step - loss: 6.7828 - accuracy: 0.1056 - val_loss: 5.2973 - val_accuracy: 0.1061 - lr: 9.7668e-04\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - ETA: 0s - loss: 4.6417 - accuracy: 0.1090\n",
      "Epoch 2: val_accuracy did not improve from 0.10611\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 4.6417 - accuracy: 0.1090 - val_loss: 4.1416 - val_accuracy: 0.1039 - lr: 9.5379e-04\n",
      "Epoch 3/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 3.6048 - accuracy: 0.1110\n",
      "Epoch 3: val_accuracy did not improve from 0.10611\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 3.6048 - accuracy: 0.1110 - val_loss: 3.3575 - val_accuracy: 0.1044 - lr: 9.3145e-04\n",
      "Epoch 4/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 3.0358 - accuracy: 0.1126\n",
      "Epoch 4: val_accuracy did not improve from 0.10611\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 3.0358 - accuracy: 0.1126 - val_loss: 2.8596 - val_accuracy: 0.1061 - lr: 9.0963e-04\n",
      "Epoch 5/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.7146 - accuracy: 0.1103\n",
      "Epoch 5: val_accuracy improved from 0.10611 to 0.11111, saving model to best_model.h5\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.7146 - accuracy: 0.1103 - val_loss: 2.6014 - val_accuracy: 0.1111 - lr: 8.8832e-04\n",
      "Epoch 6/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.5267 - accuracy: 0.1133\n",
      "Epoch 6: val_accuracy did not improve from 0.11111\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.5264 - accuracy: 0.1129 - val_loss: 2.4888 - val_accuracy: 0.1094 - lr: 8.6751e-04\n",
      "Epoch 7/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.4274 - accuracy: 0.1094\n",
      "Epoch 7: val_accuracy improved from 0.11111 to 0.11500, saving model to best_model.h5\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.4274 - accuracy: 0.1094 - val_loss: 2.4046 - val_accuracy: 0.1150 - lr: 8.4718e-04\n",
      "Epoch 8/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.3830 - accuracy: 0.1125\n",
      "Epoch 8: val_accuracy did not improve from 0.11500\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.3830 - accuracy: 0.1125 - val_loss: 2.4832 - val_accuracy: 0.0994 - lr: 8.2734e-04\n",
      "Epoch 9/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.3478 - accuracy: 0.1145\n",
      "Epoch 9: val_accuracy did not improve from 0.11500\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.3474 - accuracy: 0.1150 - val_loss: 2.3952 - val_accuracy: 0.1000 - lr: 8.0795e-04\n",
      "Epoch 10/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.3200 - accuracy: 0.1135\n",
      "Epoch 10: val_accuracy did not improve from 0.11500\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.3200 - accuracy: 0.1135 - val_loss: 2.2945 - val_accuracy: 0.1106 - lr: 7.8903e-04\n",
      "Epoch 11/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2951 - accuracy: 0.1118\n",
      "Epoch 11: val_accuracy improved from 0.11500 to 0.11667, saving model to best_model.h5\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 2.2951 - accuracy: 0.1118 - val_loss: 2.3112 - val_accuracy: 0.1167 - lr: 7.7054e-04\n",
      "Epoch 12/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2802 - accuracy: 0.1071\n",
      "Epoch 12: val_accuracy improved from 0.11667 to 0.11722, saving model to best_model.h5\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2801 - accuracy: 0.1072 - val_loss: 2.2943 - val_accuracy: 0.1172 - lr: 7.5249e-04\n",
      "Epoch 13/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2750 - accuracy: 0.1107\n",
      "Epoch 13: val_accuracy did not improve from 0.11722\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2750 - accuracy: 0.1107 - val_loss: 2.3339 - val_accuracy: 0.1150 - lr: 7.3486e-04\n",
      "Epoch 14/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2768 - accuracy: 0.1085\n",
      "Epoch 14: val_accuracy did not improve from 0.11722\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2768 - accuracy: 0.1085 - val_loss: 2.3219 - val_accuracy: 0.1150 - lr: 7.1764e-04\n",
      "Epoch 15/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2758 - accuracy: 0.1119\n",
      "Epoch 15: val_accuracy did not improve from 0.11722\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2758 - accuracy: 0.1119 - val_loss: 2.3013 - val_accuracy: 0.1172 - lr: 7.0083e-04\n",
      "Epoch 16/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2615 - accuracy: 0.1093\n",
      "Epoch 16: val_accuracy did not improve from 0.11722\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2615 - accuracy: 0.1093 - val_loss: 2.4129 - val_accuracy: 0.1161 - lr: 6.8441e-04\n",
      "Epoch 17/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2704 - accuracy: 0.1089\n",
      "Epoch 17: val_accuracy improved from 0.11722 to 0.11833, saving model to best_model.h5\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2704 - accuracy: 0.1089 - val_loss: 2.2960 - val_accuracy: 0.1183 - lr: 6.6838e-04\n",
      "Epoch 18/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2601 - accuracy: 0.1163\n",
      "Epoch 18: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2601 - accuracy: 0.1163 - val_loss: 2.3990 - val_accuracy: 0.1094 - lr: 6.5272e-04\n",
      "Epoch 19/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2623 - accuracy: 0.1056\n",
      "Epoch 19: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2627 - accuracy: 0.1057 - val_loss: 2.2926 - val_accuracy: 0.1156 - lr: 6.3743e-04\n",
      "Epoch 20/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2529 - accuracy: 0.1083\n",
      "Epoch 20: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2529 - accuracy: 0.1083 - val_loss: 2.2788 - val_accuracy: 0.1167 - lr: 6.2250e-04\n",
      "Epoch 21/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2562 - accuracy: 0.1077\n",
      "Epoch 21: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2562 - accuracy: 0.1075 - val_loss: 2.3287 - val_accuracy: 0.1139 - lr: 6.0791e-04\n",
      "Epoch 22/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2562 - accuracy: 0.1156\n",
      "Epoch 22: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 2.2562 - accuracy: 0.1156 - val_loss: 2.2601 - val_accuracy: 0.1172 - lr: 5.9367e-04\n",
      "Epoch 23/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2492 - accuracy: 0.1132\n",
      "Epoch 23: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2492 - accuracy: 0.1132 - val_loss: 2.3140 - val_accuracy: 0.1006 - lr: 5.7976e-04\n",
      "Epoch 24/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2530 - accuracy: 0.1088\n",
      "Epoch 24: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2530 - accuracy: 0.1088 - val_loss: 2.3127 - val_accuracy: 0.1161 - lr: 5.6618e-04\n",
      "Epoch 25/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2543 - accuracy: 0.1127\n",
      "Epoch 25: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2541 - accuracy: 0.1126 - val_loss: 2.3177 - val_accuracy: 0.1178 - lr: 5.5292e-04\n",
      "Epoch 26/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2512 - accuracy: 0.1113\n",
      "Epoch 26: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2512 - accuracy: 0.1110 - val_loss: 2.2676 - val_accuracy: 0.1167 - lr: 5.3996e-04\n",
      "Epoch 27/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2437 - accuracy: 0.1136\n",
      "Epoch 27: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2437 - accuracy: 0.1136 - val_loss: 2.2711 - val_accuracy: 0.1061 - lr: 5.2731e-04\n",
      "Epoch 28/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2424 - accuracy: 0.1131\n",
      "Epoch 28: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2424 - accuracy: 0.1131 - val_loss: 2.3081 - val_accuracy: 0.1156 - lr: 5.1496e-04\n",
      "Epoch 29/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2394 - accuracy: 0.1110\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00010057917097583413.\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2394 - accuracy: 0.1110 - val_loss: 2.2991 - val_accuracy: 0.1167 - lr: 5.0290e-04\n",
      "Epoch 30/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2365 - accuracy: 0.1063\n",
      "Epoch 30: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2365 - accuracy: 0.1063 - val_loss: 2.2471 - val_accuracy: 0.1161 - lr: 4.9111e-04\n",
      "Epoch 31/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2370 - accuracy: 0.1174\n",
      "Epoch 31: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 56ms/step - loss: 2.2370 - accuracy: 0.1174 - val_loss: 2.2511 - val_accuracy: 0.1167 - lr: 4.7961e-04\n",
      "Epoch 32/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2354 - accuracy: 0.1144\n",
      "Epoch 32: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 52ms/step - loss: 2.2354 - accuracy: 0.1144 - val_loss: 2.3109 - val_accuracy: 0.1161 - lr: 4.6837e-04\n",
      "Epoch 33/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2408 - accuracy: 0.1081\n",
      "Epoch 33: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2408 - accuracy: 0.1081 - val_loss: 2.2414 - val_accuracy: 0.1156 - lr: 4.5740e-04\n",
      "Epoch 34/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2323 - accuracy: 0.1152\n",
      "Epoch 34: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2324 - accuracy: 0.1156 - val_loss: 2.2509 - val_accuracy: 0.1172 - lr: 4.4668e-04\n",
      "Epoch 35/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2292 - accuracy: 0.1106\n",
      "Epoch 35: val_accuracy did not improve from 0.11833\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2292 - accuracy: 0.1107 - val_loss: 2.2416 - val_accuracy: 0.1172 - lr: 4.3622e-04\n",
      "Epoch 36/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2343 - accuracy: 0.1092\n",
      "Epoch 36: val_accuracy improved from 0.11833 to 0.11889, saving model to best_model.h5\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2343 - accuracy: 0.1092 - val_loss: 2.2233 - val_accuracy: 0.1189 - lr: 4.2600e-04\n",
      "Epoch 37/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2276 - accuracy: 0.1153\n",
      "Epoch 37: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2276 - accuracy: 0.1153 - val_loss: 2.2271 - val_accuracy: 0.1172 - lr: 4.1602e-04\n",
      "Epoch 38/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2254 - accuracy: 0.1171\n",
      "Epoch 38: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2254 - accuracy: 0.1171 - val_loss: 2.2331 - val_accuracy: 0.1150 - lr: 4.0627e-04\n",
      "Epoch 39/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2283 - accuracy: 0.1140\n",
      "Epoch 39: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2283 - accuracy: 0.1140 - val_loss: 2.2802 - val_accuracy: 0.1156 - lr: 3.9676e-04\n",
      "Epoch 40/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2320 - accuracy: 0.1146\n",
      "Epoch 40: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2320 - accuracy: 0.1146 - val_loss: 2.2335 - val_accuracy: 0.1156 - lr: 3.8746e-04\n",
      "Epoch 41/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2253 - accuracy: 0.1143\n",
      "Epoch 41: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2253 - accuracy: 0.1142 - val_loss: 2.2712 - val_accuracy: 0.1161 - lr: 3.7838e-04\n",
      "Epoch 42/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2250 - accuracy: 0.1119\n",
      "Epoch 42: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2250 - accuracy: 0.1119 - val_loss: 2.2298 - val_accuracy: 0.1161 - lr: 3.6952e-04\n",
      "Epoch 43/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2236 - accuracy: 0.1159\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.217252859845758e-05.\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2236 - accuracy: 0.1160 - val_loss: 2.2416 - val_accuracy: 0.1178 - lr: 3.6086e-04\n",
      "Epoch 44/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2211 - accuracy: 0.1103\n",
      "Epoch 44: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2211 - accuracy: 0.1103 - val_loss: 2.3189 - val_accuracy: 0.1144 - lr: 3.5241e-04\n",
      "Epoch 45/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2223 - accuracy: 0.1121\n",
      "Epoch 45: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2223 - accuracy: 0.1121 - val_loss: 2.2281 - val_accuracy: 0.1172 - lr: 3.4415e-04\n",
      "Epoch 46/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2222 - accuracy: 0.1124\n",
      "Epoch 46: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2221 - accuracy: 0.1126 - val_loss: 2.2336 - val_accuracy: 0.1167 - lr: 3.3609e-04\n",
      "Epoch 47/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2197 - accuracy: 0.1147\n",
      "Epoch 47: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2197 - accuracy: 0.1150 - val_loss: 2.3091 - val_accuracy: 0.1150 - lr: 3.2822e-04\n",
      "Epoch 48/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2199 - accuracy: 0.1114\n",
      "Epoch 48: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2199 - accuracy: 0.1114 - val_loss: 2.2318 - val_accuracy: 0.1161 - lr: 3.2053e-04\n",
      "Epoch 49/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2195 - accuracy: 0.1156\n",
      "Epoch 49: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 2.2195 - accuracy: 0.1156 - val_loss: 2.2293 - val_accuracy: 0.1161 - lr: 3.1302e-04\n",
      "Epoch 50/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2189 - accuracy: 0.1099\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.113696144893765e-05.\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2189 - accuracy: 0.1099 - val_loss: 2.2298 - val_accuracy: 0.1172 - lr: 3.0568e-04\n",
      "Epoch 51/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2218 - accuracy: 0.1093\n",
      "Epoch 51: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2218 - accuracy: 0.1093 - val_loss: 2.2755 - val_accuracy: 0.1161 - lr: 2.9852e-04\n",
      "Epoch 52/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2206 - accuracy: 0.1138\n",
      "Epoch 52: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2206 - accuracy: 0.1138 - val_loss: 2.2722 - val_accuracy: 0.0994 - lr: 2.9153e-04\n",
      "Epoch 53/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2162 - accuracy: 0.1140\n",
      "Epoch 53: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2162 - accuracy: 0.1140 - val_loss: 2.2580 - val_accuracy: 0.1167 - lr: 2.8470e-04\n",
      "Epoch 54/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2203 - accuracy: 0.1116\n",
      "Epoch 54: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 11s 49ms/step - loss: 2.2203 - accuracy: 0.1117 - val_loss: 2.2421 - val_accuracy: 0.1167 - lr: 2.7803e-04\n",
      "Epoch 55/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2183 - accuracy: 0.1152\n",
      "Epoch 55: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2182 - accuracy: 0.1151 - val_loss: 2.2168 - val_accuracy: 0.1044 - lr: 2.7152e-04\n",
      "Epoch 56/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2136 - accuracy: 0.1093\n",
      "Epoch 56: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2136 - accuracy: 0.1093 - val_loss: 2.2492 - val_accuracy: 0.1172 - lr: 2.6516e-04\n",
      "Epoch 57/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2169 - accuracy: 0.1137\n",
      "Epoch 57: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2169 - accuracy: 0.1136 - val_loss: 2.2209 - val_accuracy: 0.1167 - lr: 2.5894e-04\n",
      "Epoch 58/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2147 - accuracy: 0.1116\n",
      "Epoch 58: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2147 - accuracy: 0.1112 - val_loss: 2.2434 - val_accuracy: 0.0994 - lr: 2.5288e-04\n",
      "Epoch 59/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2125 - accuracy: 0.1187\n",
      "Epoch 59: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2125 - accuracy: 0.1187 - val_loss: 2.2440 - val_accuracy: 0.1172 - lr: 2.4695e-04\n",
      "Epoch 60/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2175 - accuracy: 0.1138\n",
      "Epoch 60: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 52ms/step - loss: 2.2175 - accuracy: 0.1138 - val_loss: 2.2490 - val_accuracy: 0.1144 - lr: 2.4117e-04\n",
      "Epoch 61/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2132 - accuracy: 0.1108\n",
      "Epoch 61: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2132 - accuracy: 0.1108 - val_loss: 2.2320 - val_accuracy: 0.1150 - lr: 2.3552e-04\n",
      "Epoch 62/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2122 - accuracy: 0.1133\n",
      "Epoch 62: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2122 - accuracy: 0.1133 - val_loss: 2.2152 - val_accuracy: 0.1022 - lr: 2.3000e-04\n",
      "Epoch 63/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2092 - accuracy: 0.1164\n",
      "Epoch 63: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.2092 - accuracy: 0.1164 - val_loss: 2.2339 - val_accuracy: 0.1161 - lr: 2.2461e-04\n",
      "Epoch 64/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2115 - accuracy: 0.1128\n",
      "Epoch 64: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2115 - accuracy: 0.1128 - val_loss: 2.2328 - val_accuracy: 0.1011 - lr: 2.1935e-04\n",
      "Epoch 65/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2117 - accuracy: 0.1102\n",
      "Epoch 65: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 56ms/step - loss: 2.2117 - accuracy: 0.1104 - val_loss: 2.2628 - val_accuracy: 0.1161 - lr: 2.1421e-04\n",
      "Epoch 66/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2112 - accuracy: 0.1192\n",
      "Epoch 66: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2112 - accuracy: 0.1192 - val_loss: 2.2199 - val_accuracy: 0.1017 - lr: 2.0919e-04\n",
      "Epoch 67/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2104 - accuracy: 0.1115\n",
      "Epoch 67: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2104 - accuracy: 0.1115 - val_loss: 2.2146 - val_accuracy: 0.0994 - lr: 2.0429e-04\n",
      "Epoch 68/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2107 - accuracy: 0.1095\n",
      "Epoch 68: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2108 - accuracy: 0.1097 - val_loss: 2.2142 - val_accuracy: 0.1033 - lr: 1.9951e-04\n",
      "Epoch 69/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2065 - accuracy: 0.1154\n",
      "Epoch 69: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2065 - accuracy: 0.1154 - val_loss: 2.2188 - val_accuracy: 0.1161 - lr: 1.9483e-04\n",
      "Epoch 70/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2066 - accuracy: 0.1123\n",
      "Epoch 70: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2066 - accuracy: 0.1121 - val_loss: 2.2392 - val_accuracy: 0.1022 - lr: 1.9027e-04\n",
      "Epoch 71/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2061 - accuracy: 0.1136\n",
      "Epoch 71: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2061 - accuracy: 0.1136 - val_loss: 2.2235 - val_accuracy: 0.1167 - lr: 1.8581e-04\n",
      "Epoch 72/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2071 - accuracy: 0.1142\n",
      "Epoch 72: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2071 - accuracy: 0.1142 - val_loss: 2.2592 - val_accuracy: 0.1156 - lr: 1.8146e-04\n",
      "Epoch 73/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2075 - accuracy: 0.1136\n",
      "Epoch 73: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2075 - accuracy: 0.1136 - val_loss: 2.2558 - val_accuracy: 0.1006 - lr: 1.7721e-04\n",
      "Epoch 74/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2087 - accuracy: 0.1144\n",
      "Epoch 74: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2087 - accuracy: 0.1144 - val_loss: 2.2255 - val_accuracy: 0.0994 - lr: 1.7305e-04\n",
      "Epoch 75/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2075 - accuracy: 0.1138\n",
      "Epoch 75: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2075 - accuracy: 0.1136 - val_loss: 2.2074 - val_accuracy: 0.1039 - lr: 1.6900e-04\n",
      "Epoch 76/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2055 - accuracy: 0.1131\n",
      "Epoch 76: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2055 - accuracy: 0.1131 - val_loss: 2.2130 - val_accuracy: 0.1017 - lr: 1.6504e-04\n",
      "Epoch 77/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2045 - accuracy: 0.1132\n",
      "Epoch 77: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2045 - accuracy: 0.1132 - val_loss: 2.2122 - val_accuracy: 0.1011 - lr: 1.6117e-04\n",
      "Epoch 78/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2050 - accuracy: 0.1167\n",
      "Epoch 78: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2050 - accuracy: 0.1167 - val_loss: 2.2156 - val_accuracy: 0.1022 - lr: 1.5740e-04\n",
      "Epoch 79/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2062 - accuracy: 0.1124\n",
      "Epoch 79: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2062 - accuracy: 0.1124 - val_loss: 2.2140 - val_accuracy: 0.1000 - lr: 1.5371e-04\n",
      "Epoch 80/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2041 - accuracy: 0.1114\n",
      "Epoch 80: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2041 - accuracy: 0.1114 - val_loss: 2.2307 - val_accuracy: 0.1011 - lr: 1.5011e-04\n",
      "Epoch 81/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2056 - accuracy: 0.1176\n",
      "Epoch 81: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2056 - accuracy: 0.1176 - val_loss: 2.2130 - val_accuracy: 0.1167 - lr: 1.4659e-04\n",
      "Epoch 82/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2056 - accuracy: 0.1156\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 2.863187692128122e-05.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.2056 - accuracy: 0.1156 - val_loss: 2.2139 - val_accuracy: 0.1167 - lr: 1.4316e-04\n",
      "Epoch 83/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2046 - accuracy: 0.1117\n",
      "Epoch 83: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2046 - accuracy: 0.1117 - val_loss: 2.2042 - val_accuracy: 0.1183 - lr: 1.3981e-04\n",
      "Epoch 84/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2050 - accuracy: 0.1164\n",
      "Epoch 84: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2050 - accuracy: 0.1164 - val_loss: 2.2161 - val_accuracy: 0.1156 - lr: 1.3653e-04\n",
      "Epoch 85/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2044 - accuracy: 0.1139\n",
      "Epoch 85: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2044 - accuracy: 0.1139 - val_loss: 2.2056 - val_accuracy: 0.1000 - lr: 1.3333e-04\n",
      "Epoch 86/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2035 - accuracy: 0.1115\n",
      "Epoch 86: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2034 - accuracy: 0.1115 - val_loss: 2.2108 - val_accuracy: 0.1150 - lr: 1.3021e-04\n",
      "Epoch 87/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2033 - accuracy: 0.1168\n",
      "Epoch 87: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2033 - accuracy: 0.1168 - val_loss: 2.2208 - val_accuracy: 0.1167 - lr: 1.2716e-04\n",
      "Epoch 88/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2054 - accuracy: 0.1160\n",
      "Epoch 88: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2054 - accuracy: 0.1160 - val_loss: 2.2147 - val_accuracy: 0.1172 - lr: 1.2418e-04\n",
      "Epoch 89/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2041 - accuracy: 0.1128\n",
      "Epoch 89: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2041 - accuracy: 0.1128 - val_loss: 2.2091 - val_accuracy: 0.1167 - lr: 1.2127e-04\n",
      "Epoch 90/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2034 - accuracy: 0.1193\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.3685705673415215e-05.\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2034 - accuracy: 0.1192 - val_loss: 2.2054 - val_accuracy: 0.1017 - lr: 1.1843e-04\n",
      "Epoch 91/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2032 - accuracy: 0.1128\n",
      "Epoch 91: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2032 - accuracy: 0.1128 - val_loss: 2.2043 - val_accuracy: 0.1022 - lr: 1.1565e-04\n",
      "Epoch 92/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2023 - accuracy: 0.1144\n",
      "Epoch 92: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2023 - accuracy: 0.1144 - val_loss: 2.2076 - val_accuracy: 0.1000 - lr: 1.1294e-04\n",
      "Epoch 93/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2010 - accuracy: 0.1138\n",
      "Epoch 93: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2010 - accuracy: 0.1138 - val_loss: 2.2114 - val_accuracy: 0.1000 - lr: 1.1030e-04\n",
      "Epoch 94/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2020 - accuracy: 0.1144\n",
      "Epoch 94: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2020 - accuracy: 0.1144 - val_loss: 2.2175 - val_accuracy: 0.1006 - lr: 1.0771e-04\n",
      "Epoch 95/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2016 - accuracy: 0.1121\n",
      "Epoch 95: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2016 - accuracy: 0.1121 - val_loss: 2.2140 - val_accuracy: 0.1011 - lr: 1.0519e-04\n",
      "Epoch 96/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2028 - accuracy: 0.1123\n",
      "Epoch 96: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2028 - accuracy: 0.1126 - val_loss: 2.2113 - val_accuracy: 0.1022 - lr: 1.0273e-04\n",
      "Epoch 97/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2018 - accuracy: 0.1147\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 2.006403374252841e-05.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 14s 62ms/step - loss: 2.2018 - accuracy: 0.1147 - val_loss: 2.2084 - val_accuracy: 0.1028 - lr: 1.0032e-04\n",
      "Epoch 98/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2026 - accuracy: 0.1138\n",
      "Epoch 98: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2026 - accuracy: 0.1138 - val_loss: 2.2009 - val_accuracy: 0.1017 - lr: 9.7970e-05\n",
      "Epoch 99/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2022 - accuracy: 0.1136\n",
      "Epoch 99: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 56ms/step - loss: 2.2022 - accuracy: 0.1136 - val_loss: 2.2234 - val_accuracy: 0.1006 - lr: 9.5675e-05\n",
      "Epoch 100/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2031 - accuracy: 0.1150\n",
      "Epoch 100: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2031 - accuracy: 0.1150 - val_loss: 2.2159 - val_accuracy: 0.1006 - lr: 9.3433e-05\n",
      "Epoch 101/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2024 - accuracy: 0.1133\n",
      "Epoch 101: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.2024 - accuracy: 0.1133 - val_loss: 2.2125 - val_accuracy: 0.1011 - lr: 9.1244e-05\n",
      "Epoch 102/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2007 - accuracy: 0.1171\n",
      "Epoch 102: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2007 - accuracy: 0.1171 - val_loss: 2.2144 - val_accuracy: 0.1017 - lr: 8.9107e-05\n",
      "Epoch 103/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2014 - accuracy: 0.1157\n",
      "Epoch 103: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.2014 - accuracy: 0.1157 - val_loss: 2.2215 - val_accuracy: 0.1022 - lr: 8.7019e-05\n",
      "Epoch 104/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2026 - accuracy: 0.1143\n",
      "Epoch 104: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2025 - accuracy: 0.1140 - val_loss: 2.2177 - val_accuracy: 0.1017 - lr: 8.4981e-05\n",
      "Epoch 105/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2018 - accuracy: 0.1151\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.659796107560396e-05.\n",
      "\n",
      "Epoch 105: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2018 - accuracy: 0.1151 - val_loss: 2.2068 - val_accuracy: 0.1000 - lr: 8.2990e-05\n",
      "Epoch 106/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2009 - accuracy: 0.1132\n",
      "Epoch 106: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2009 - accuracy: 0.1132 - val_loss: 2.2159 - val_accuracy: 0.1011 - lr: 8.1046e-05\n",
      "Epoch 107/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2010 - accuracy: 0.1149\n",
      "Epoch 107: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2010 - accuracy: 0.1149 - val_loss: 2.2184 - val_accuracy: 0.1000 - lr: 7.9147e-05\n",
      "Epoch 108/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2025 - accuracy: 0.1140\n",
      "Epoch 108: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2023 - accuracy: 0.1140 - val_loss: 2.2091 - val_accuracy: 0.1017 - lr: 7.7293e-05\n",
      "Epoch 109/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.1135\n",
      "Epoch 109: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.2005 - accuracy: 0.1135 - val_loss: 2.2114 - val_accuracy: 0.1028 - lr: 7.5482e-05\n",
      "Epoch 110/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2015 - accuracy: 0.1122\n",
      "Epoch 110: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.2015 - accuracy: 0.1122 - val_loss: 2.2007 - val_accuracy: 0.1011 - lr: 7.3714e-05\n",
      "Epoch 111/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.2010 - accuracy: 0.1129\n",
      "Epoch 111: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2010 - accuracy: 0.1126 - val_loss: 2.2018 - val_accuracy: 0.1022 - lr: 7.1987e-05\n",
      "Epoch 112/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2009 - accuracy: 0.1124\n",
      "Epoch 112: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.2009 - accuracy: 0.1124 - val_loss: 2.2068 - val_accuracy: 0.1022 - lr: 7.0300e-05\n",
      "Epoch 113/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2003 - accuracy: 0.1135\n",
      "Epoch 113: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2003 - accuracy: 0.1135 - val_loss: 2.2085 - val_accuracy: 0.1017 - lr: 6.8653e-05\n",
      "Epoch 114/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1999 - accuracy: 0.1138\n",
      "Epoch 114: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.1999 - accuracy: 0.1138 - val_loss: 2.2023 - val_accuracy: 0.1028 - lr: 6.7045e-05\n",
      "Epoch 115/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1988 - accuracy: 0.1160\n",
      "Epoch 115: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1988 - accuracy: 0.1160 - val_loss: 2.2002 - val_accuracy: 0.1050 - lr: 6.5474e-05\n",
      "Epoch 116/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1995 - accuracy: 0.1169\n",
      "Epoch 116: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1995 - accuracy: 0.1169 - val_loss: 2.2036 - val_accuracy: 0.1006 - lr: 6.3940e-05\n",
      "Epoch 117/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2012 - accuracy: 0.1144\n",
      "Epoch 117: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.2012 - accuracy: 0.1144 - val_loss: 2.2072 - val_accuracy: 0.1017 - lr: 6.2442e-05\n",
      "Epoch 118/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2001 - accuracy: 0.1104\n",
      "Epoch 118: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.2001 - accuracy: 0.1104 - val_loss: 2.2194 - val_accuracy: 0.1156 - lr: 6.0980e-05\n",
      "Epoch 119/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1995 - accuracy: 0.1236\n",
      "Epoch 119: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1995 - accuracy: 0.1236 - val_loss: 2.2086 - val_accuracy: 0.1028 - lr: 5.9551e-05\n",
      "Epoch 120/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1990 - accuracy: 0.1147\n",
      "Epoch 120: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1990 - accuracy: 0.1147 - val_loss: 2.2002 - val_accuracy: 0.1039 - lr: 5.8156e-05\n",
      "Epoch 121/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.1179\n",
      "Epoch 121: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.2005 - accuracy: 0.1179 - val_loss: 2.2061 - val_accuracy: 0.1022 - lr: 5.6793e-05\n",
      "Epoch 122/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1997 - accuracy: 0.1139\n",
      "Epoch 122: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1997 - accuracy: 0.1139 - val_loss: 2.1985 - val_accuracy: 0.1039 - lr: 5.5463e-05\n",
      "Epoch 123/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1993 - accuracy: 0.1168\n",
      "Epoch 123: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.1993 - accuracy: 0.1168 - val_loss: 2.1988 - val_accuracy: 0.1033 - lr: 5.4164e-05\n",
      "Epoch 124/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1991 - accuracy: 0.1157\n",
      "Epoch 124: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1991 - accuracy: 0.1157 - val_loss: 2.2032 - val_accuracy: 0.1028 - lr: 5.2895e-05\n",
      "Epoch 125/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1981 - accuracy: 0.1106\n",
      "Epoch 125: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1981 - accuracy: 0.1106 - val_loss: 2.2136 - val_accuracy: 0.0994 - lr: 5.1655e-05\n",
      "Epoch 126/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1987 - accuracy: 0.1115\n",
      "Epoch 126: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 52ms/step - loss: 2.1987 - accuracy: 0.1115 - val_loss: 2.2126 - val_accuracy: 0.1028 - lr: 5.0445e-05\n",
      "Epoch 127/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1987 - accuracy: 0.1115\n",
      "Epoch 127: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 2.1987 - accuracy: 0.1115 - val_loss: 2.2064 - val_accuracy: 0.1039 - lr: 4.9263e-05\n",
      "Epoch 128/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1977 - accuracy: 0.1197\n",
      "Epoch 128: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.1977 - accuracy: 0.1197 - val_loss: 2.2193 - val_accuracy: 0.1006 - lr: 4.8109e-05\n",
      "Epoch 129/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1982 - accuracy: 0.1169\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 9.396458335686476e-06.\n",
      "\n",
      "Epoch 129: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1982 - accuracy: 0.1169 - val_loss: 2.2151 - val_accuracy: 0.1006 - lr: 4.6982e-05\n",
      "Epoch 130/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1977 - accuracy: 0.1201\n",
      "Epoch 130: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1977 - accuracy: 0.1201 - val_loss: 2.2098 - val_accuracy: 0.1022 - lr: 4.5882e-05\n",
      "Epoch 131/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1992 - accuracy: 0.1168\n",
      "Epoch 131: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1992 - accuracy: 0.1168 - val_loss: 2.2086 - val_accuracy: 0.1022 - lr: 4.4807e-05\n",
      "Epoch 132/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1984 - accuracy: 0.1154\n",
      "Epoch 132: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1984 - accuracy: 0.1154 - val_loss: 2.2071 - val_accuracy: 0.1006 - lr: 4.3757e-05\n",
      "Epoch 133/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1976 - accuracy: 0.1163\n",
      "Epoch 133: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 56ms/step - loss: 2.1976 - accuracy: 0.1163 - val_loss: 2.2158 - val_accuracy: 0.1028 - lr: 4.2732e-05\n",
      "Epoch 134/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1981 - accuracy: 0.1179\n",
      "Epoch 134: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1981 - accuracy: 0.1179 - val_loss: 2.2094 - val_accuracy: 0.1039 - lr: 4.1731e-05\n",
      "Epoch 135/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.1972 - accuracy: 0.1177\n",
      "Epoch 135: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 58ms/step - loss: 2.1971 - accuracy: 0.1176 - val_loss: 2.2218 - val_accuracy: 0.1011 - lr: 4.0753e-05\n",
      "Epoch 136/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1970 - accuracy: 0.1160\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 7.959690265124664e-06.\n",
      "\n",
      "Epoch 136: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1970 - accuracy: 0.1160 - val_loss: 2.2035 - val_accuracy: 0.1039 - lr: 3.9798e-05\n",
      "Epoch 137/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.1971 - accuracy: 0.1155\n",
      "Epoch 137: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.1969 - accuracy: 0.1160 - val_loss: 2.2124 - val_accuracy: 0.1017 - lr: 3.8866e-05\n",
      "Epoch 138/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1966 - accuracy: 0.1125\n",
      "Epoch 138: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1966 - accuracy: 0.1125 - val_loss: 2.2078 - val_accuracy: 0.1178 - lr: 3.7956e-05\n",
      "Epoch 139/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1973 - accuracy: 0.1133\n",
      "Epoch 139: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1973 - accuracy: 0.1133 - val_loss: 2.2113 - val_accuracy: 0.1000 - lr: 3.7066e-05\n",
      "Epoch 140/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.1966 - accuracy: 0.1191\n",
      "Epoch 140: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1965 - accuracy: 0.1194 - val_loss: 2.1972 - val_accuracy: 0.1044 - lr: 3.6198e-05\n",
      "Epoch 141/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1980 - accuracy: 0.1160\n",
      "Epoch 141: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 2.1980 - accuracy: 0.1160 - val_loss: 2.2003 - val_accuracy: 0.1044 - lr: 3.5350e-05\n",
      "Epoch 142/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.1956 - accuracy: 0.1182\n",
      "Epoch 142: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1956 - accuracy: 0.1181 - val_loss: 2.1990 - val_accuracy: 0.1039 - lr: 3.4522e-05\n",
      "Epoch 143/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1963 - accuracy: 0.1128\n",
      "Epoch 143: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1963 - accuracy: 0.1128 - val_loss: 2.2029 - val_accuracy: 0.1044 - lr: 3.3713e-05\n",
      "Epoch 144/150\n",
      "224/225 [============================>.] - ETA: 0s - loss: 2.1961 - accuracy: 0.1145\n",
      "Epoch 144: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1960 - accuracy: 0.1149 - val_loss: 2.1988 - val_accuracy: 0.1044 - lr: 3.2923e-05\n",
      "Epoch 145/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1958 - accuracy: 0.1161\n",
      "Epoch 145: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1958 - accuracy: 0.1161 - val_loss: 2.2043 - val_accuracy: 0.1017 - lr: 3.2152e-05\n",
      "Epoch 146/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1962 - accuracy: 0.1124\n",
      "Epoch 146: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1962 - accuracy: 0.1124 - val_loss: 2.2038 - val_accuracy: 0.1017 - lr: 3.1399e-05\n",
      "Epoch 147/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1955 - accuracy: 0.1171\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 6.132623821031302e-06.\n",
      "\n",
      "Epoch 147: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 2.1955 - accuracy: 0.1171 - val_loss: 2.1983 - val_accuracy: 0.1039 - lr: 3.0663e-05\n",
      "Epoch 148/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1956 - accuracy: 0.1168\n",
      "Epoch 148: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 2.1956 - accuracy: 0.1168 - val_loss: 2.1954 - val_accuracy: 0.1056 - lr: 2.9945e-05\n",
      "Epoch 149/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1946 - accuracy: 0.1185\n",
      "Epoch 149: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 2.1946 - accuracy: 0.1185 - val_loss: 2.2023 - val_accuracy: 0.1061 - lr: 2.9243e-05\n",
      "Epoch 150/150\n",
      "225/225 [==============================] - ETA: 0s - loss: 2.1943 - accuracy: 0.1178\n",
      "Epoch 150: val_accuracy did not improve from 0.11889\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 2.1943 - accuracy: 0.1178 - val_loss: 2.1978 - val_accuracy: 0.1067 - lr: 2.8558e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the enhanced model\n",
    "model, history = train_enhanced_model(X_train, y_train, X_test, y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7f4d7f8-6472-4cee-b214-9eb612b6e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 13ms/step - loss: 2.2009 - accuracy: 0.1006\n",
      "Test accuracy: 0.1006\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f09951a0-d70c-4f8d-8ec3-1d5dcf80b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(X_train.shape[1], num_classes)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f82c1eba-0467-4497-8ced-372429a93da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.7878 - accuracy: 0.1733 - val_loss: 1.7894 - val_accuracy: 0.1517\n",
      "Epoch 2/150\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 1.7890 - accuracy: 0.1717 - val_loss: 1.7878 - val_accuracy: 0.1683\n",
      "Epoch 3/150\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.7875 - accuracy: 0.1679 - val_loss: 1.7909 - val_accuracy: 0.1492\n",
      "Epoch 4/150\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.7880 - accuracy: 0.1752 - val_loss: 1.7880 - val_accuracy: 0.1525\n",
      "Epoch 5/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7883 - accuracy: 0.1692 - val_loss: 1.7864 - val_accuracy: 0.1692\n",
      "Epoch 6/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7854 - accuracy: 0.1752 - val_loss: 1.7849 - val_accuracy: 0.1542\n",
      "Epoch 7/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7862 - accuracy: 0.1692 - val_loss: 1.7852 - val_accuracy: 0.1542\n",
      "Epoch 8/150\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.7860 - accuracy: 0.1727 - val_loss: 1.7838 - val_accuracy: 0.1558\n",
      "Epoch 9/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7866 - accuracy: 0.1729 - val_loss: 1.7877 - val_accuracy: 0.1533\n",
      "Epoch 10/150\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.7849 - accuracy: 0.1740 - val_loss: 1.7852 - val_accuracy: 0.1550\n",
      "Epoch 11/150\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.7840 - accuracy: 0.1769 - val_loss: 1.7861 - val_accuracy: 0.1550\n",
      "Epoch 12/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7860 - accuracy: 0.1787 - val_loss: 1.7839 - val_accuracy: 0.1575\n",
      "Epoch 13/150\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.7825 - accuracy: 0.1769 - val_loss: 1.7833 - val_accuracy: 0.1575\n",
      "Epoch 14/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7847 - accuracy: 0.1723 - val_loss: 1.7837 - val_accuracy: 0.1558\n",
      "Epoch 15/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7840 - accuracy: 0.1713 - val_loss: 1.7827 - val_accuracy: 0.1567\n",
      "Epoch 16/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7858 - accuracy: 0.1700 - val_loss: 1.7831 - val_accuracy: 0.1558\n",
      "Epoch 17/150\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.7824 - accuracy: 0.1740 - val_loss: 1.7814 - val_accuracy: 0.1567\n",
      "Epoch 18/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7820 - accuracy: 0.1760 - val_loss: 1.7836 - val_accuracy: 0.1567\n",
      "Epoch 19/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7830 - accuracy: 0.1777 - val_loss: 1.7839 - val_accuracy: 0.1558\n",
      "Epoch 20/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7823 - accuracy: 0.1777 - val_loss: 1.7837 - val_accuracy: 0.1567\n",
      "Epoch 21/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7812 - accuracy: 0.1740 - val_loss: 1.7833 - val_accuracy: 0.1567\n",
      "Epoch 22/150\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.7825 - accuracy: 0.1646 - val_loss: 1.7821 - val_accuracy: 0.1575\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,validation_data=(X_test, y_test),epochs=150,batch_size=32,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23ba6167-b22e-4c10-99f7-8584f3a02881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 1.7880 - accuracy: 0.1517\n",
      "Test accuracy: 0.1517\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53fb9e-6364-4e8a-a520-55ae846282e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86807c-5eb3-4e05-926f-38095aff53e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
